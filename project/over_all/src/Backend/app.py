from flask import Flask, request, jsonify
import os
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
import hashlib
import time

app = Flask(__name__)


@app.route('/')
def home():
    return jsonify({"message": "Welcome to the API!"})
# Initialize API route
@app.route('/generate-questions', methods=['POST'])
def generate_questions():
    # Parse the JSON request data
    data = request.json
    context = data.get('context')
    num_questions = data.get('num_questions')
    question_type = data.get('question_type')
    difficulty = data.get('difficulty')
    distribution = data.get('distribution')
    generate_with_answers = data.get('generate_with_answers')
    
    # Initialize the model and prompt
    model_name = 'llama3'  # Assuming llama3 model
    temp = 0.7  # Set temperature (can be dynamic based on input)
    
    model, prompt = initialize_the_model(model_name, temp)

    # Generate the prompt template for question generation
    prompt_template = generate_prompt_forQAGen(distribution, [question_type], generate_with_answers)

    # Prepare the final QA chain
    qa_chain = retrieval_qa_chain(context, model, prompt_template)

    # Generate the questions
    result = generate_answer(context, qa_chain)
    
    # Format the response
    response = {
        "questions": result,
        "answers": None if generate_with_answers == "NO" else "Answer text generated by model"
    }
    
    return jsonify(response)

# Initialize the Model
def initialize_the_model(model_name, temp):
    start_time = time.time()
    print(f"Selected model is {model_name} and Temperature is {temp}")
    
    # For Llama3 and similar models, we initialize ChatOllama
    model = ChatOllama(model=model_name, temperature=temp)
    
    # Create the prompt template for generating questions
    prompt_template = """Answer the question based only on the following context:
    {context}
    Question: {question}.
    """
    prompt = PromptTemplate.from_template(template=prompt_template)
    
    print("Model Is Initialized")
    end_time = time.time()
    print(f"Time taken to initialize the model: {end_time - start_time:.2f} seconds")
    
    return model, prompt

# Function to generate a prompt for QA Generation
def generate_prompt_forQAGen(distribution, selected_Qtype, questionsWithAns):
    query = """
    Based on the provided context, generate questions according to the following instructions. Each category must be addressed separately and in order.
 
    Context: {context}
 
    Generate questions in the following format, ensuring the exact number for each category is met:
    """
 
    for qtype in selected_Qtype:
        query += f"\n\t{qtype} :\n\n"
        for level in ['Easy', 'Medium', 'Hard']:
            if distribution.get(level, 0) > 0:
                query += f"""\t- Generate {distribution.get(level)} {level.lower()} level {qtype} \n
                            - Ensure you generate exactly {distribution.get(level)} questions, no more, no less.\n\n"""
 
    if questionsWithAns == "NO":
        query += "\n\tImportant: Do not provide answers to the questions.\n"
    else:
        query += "\n\tImportant: Provide answers to all questions.\n"
 
    query += """
    Additional Instructions:
    - Generate exactly the number of questions specified for each category and level.
    - Ensure questions cover different aspects of the content.
    - Maintain the categorization as shown above.
    - Do not skip any specified category or level.
 
    Begin generating questions now: generate the response exactly as mentioned in the prompt without fail.
    """
    return PromptTemplate.from_template(query)

# Generate an answer using the model
def generate_answer(context, qa_chain):
    start_time = time.time()
    result = qa_chain.invoke(context)
    print("Answer is generated!!")
    end_time = time.time()
    print(f"Time taken by generate_answer: {end_time - start_time:.2f} seconds")
    return result

if __name__ == '__main__':
    app.run(port=5000)
